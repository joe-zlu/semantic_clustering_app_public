# app.py
import streamlit as st
from modules.nav import Navbar

def main():

    Navbar()

    st.set_page_config(
        page_title="Semantic Clustering App with Llama.cpp",
        page_icon="üß©",
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.title("üß© Semantic Clustering and Keyword Extraction Application")

    st.markdown("""
This Streamlit application helps to turn unstructured text into structured insights using LLM-generated embeddings (i.e. numerical representations of meaning), semantic clustering, keyword extraction, and LLM prompt generation (for use with Microsoft Copilot). Follow the steps below to get the most out of the tool.

---

## üìå **Overview of Workflow**

The app is organised into **6 sequential pages** (accessible via the sidebar):

Step 1. **Data Management** ‚Üí Upload & generate or load word embeddings
Step 2. **Semantic Clustering** ‚Üí Group similar texts using agglomerative clustering
Step 3a Part 1. **Cluster Keyword Extraction** ‚Üí Extract representative keywords per cluster using BERTopic language processing algorithm
Step 3a Part 2. **Document-level Keywords** ‚Üí Analyse which keywords are most relevant to each document
Step 3b. **LLM Prompt Generation** ‚Üí Auto-generate prompts for LLMs to analyse your clustered data
Step 4. **LLM Prompt Execution** ‚Üí Run generated prompts against local LLM server and collect responses

> ‚úÖ **Tip**: Always proceed in order‚Äîeach step builds on the previous one.

---

## üîπ Step 1: **Data Management**

### What it does:
- Upload a CSV file containing text chunks
- Generate embeddings using a local Llama.cpp server
- Save embeddings to server/load embeddings from server

### How to use:

1. **Configure the Llama.cpp server URL** in the sidebar. Common options:
   - `http://evlchdprs02.edw.health:8889/` ‚Üí Gemma 300M (fast)
   - `http://evlchdprs02.edw.health:8890/` ‚Üí Qwen3 600M (balanced)
   - `http://evlchdprs02.edw.health:8891/` ‚Üí Qwen3 4B (high quality, slower)

2. **Upload a CSV file** with a "text" column (e.g., survey responses, policy comments, clinical notes) and a "text_id" column (e.g., row number, respondent ID) 

3. **Select the text column** to embed.

4. **Click ‚ÄúGenerate Embeddings‚Äù** ‚Üí The app will call your Llama.cpp server to create vector embeddings for each text.

5. **(Optional)** Compare two embeddings using cosine similarity to test the embeddings are correct.

6. **Save your embeddings**:
   - Click **‚ÄúDownload Embeddings as JSON‚Äù** to save locally
   - Or **‚ÄúSave to Server‚Äù** for later use

---

## üîπ Step 2: **Semantic Clustering**

### What it does:
- Group similar texts using hierarchical (agglomerative) clustering
- Visualise clusters using Principal Component Analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE)
- Explore results via dendrograms or scatter plots

### How to use:

> ‚ö†Ô∏è **You must have embeddings from Step 1 loaded.**

Choose **one** of two clustering methods:

### A. **By Number of Clusters**
- Set your desired number of clusters (e.g., 5‚Äì10), each cluster represents a semantic key theme
- Choose a linkage method (`ward` is usually best for compact clusters)
- Click **‚ÄúPerform Clustering‚Äù**
- View interactive 2D plot and cluster-text mapping

### B. **By Tree Cutting (Distance Threshold)**
- Adjust the **Tree Height** slider to control granularity of clusters
  - Lower = more clusters, higher = fewer clusters
- View the **interactive dendrogram** ‚Üí red line shows cut point
- Explore clusters in the same way as above

> üì§ **Export**: Download results as JSON to preserve cluster assignments.

---

## üîπ Step 3 Option A - Part 1: **Cluster Keyword Extraction**

### What it does:
- Uses **BERTopic‚Äôs c-TF-IDF** to extract meaningful keywords for each cluster
- Shows which terms best represent each cluster of texts

### How to use:

> ‚ö†Ô∏è **You must have performed clustering in Step 2.**

1. Adjust keyword parameters:
   - **Top N Words**: How many keywords per cluster (5‚Äì20 recommended)
   - **N-gram Range**: 1‚Äì2 for phrases like ‚Äúmachine learning‚Äù
   - **Min/Max DF**: Filter rare or overly common words

2. Click **‚ÄúExtract Keywords using BERTopic‚Äù**

3. View:
   - Summary table of top keywords per cluster
   - Full list of texts with their cluster‚Äôs keywords
   - Detailed c-TF-IDF scores (expand section)

4. **Download** results as JSON or CSV

> üí° Use this to **label your clusters** (e.g., ‚ÄúCluster 3 = Regulatory Concerns‚Äù).

---

## üîπ Step 3 Option A Part 2: **Document-level Keywords**

### What it does:
- For each document **within a cluster**, shows **which keywords are most relevant**
- Uses **TF-IDF** or **frequency** to score relevance

### How to use:

> ‚ö†Ô∏è **Requires BERTopic results from Step 3 Option A Part 1.**

1. Choose a **relevance method**:
   - `tfidf` ‚Üí better for distinctive terms (recommended)
   - `frequency` ‚Üí simpler word counts

2. Set a **relevance threshold** (e.g., 0.1) to filter out weak matches

3. Click **‚ÄúAnalyse Document-Topic Relevance‚Äù**

4. Explore:
   - Cluster summary (avg. relevance per keyword)
   - Per-document keyword scores
   - Full text with relevance-highlighted terms

5. **Download** detailed results

> üí° Helps identify **outliers** or **multi-theme documents**.

---

## üîπ Step 3 Option B Part 1: **LLM Prompt Generation**

### What it does:
- Auto-generates ready-to-use prompts for use with Microsoft Copilot to gather additional insights (e.g. cluster summarisation, key issue tagging)
- Each prompt contains:
  - Your instruction
  - Optional issue tags
  - Clustered texts (split to respect word limits)

### How to use:

1. **Write your analysis instruction**, e.g.:
   > ‚ÄúSummarise key concerns in these texts and suggest policy recommendations.‚Äù

2. **(Optional)** Provide a **comma-separated list of issue tags** for categorisation.

3. Set a **word limit** (e.g., 3000 words) ‚Äî large clusters will be split into multiple prompts.

4. Click **‚ÄúGenerate LLM Prompts‚Äù**

5. View or **copy prompts**:
   - Use the `</>` **code block** to copy to clipboard
   - Or download as **JSON** or **ZIP of .txt files**

> üí° Paste these prompts directly into your LLM interface for batch analysis!

---

## üîπ Step 3 Option B Part 2: **LLM Prompt Execution**

### What it does:
- Execute generated prompts against a local LLM server using OpenAI-compatible endpoints
- Collect and display prompt-response pairs for analysis
- Support both chat completion and completion formats
- Provide bulk execution capabilities for all prompts

### How to use:

> ‚ö†Ô∏è **You must have generated prompts in Step 3b.**

1. **Configure your LLM server** in the sidebar:
   - Server URL (default: `http://localhost:8080`)
   - Optional API key if required
   - Model name and generation parameters

2. **Select prompts to execute**:
   - Choose individual prompts from specific clusters
   - Or select "All" to see all available prompts

3. **Execute prompts**:
   - Click **‚ñ∂Ô∏è Run** for individual prompts
   - Use **üöÄ Execute All Prompts** for bulk execution
   - Monitor execution progress and timing

4. **Review responses**:
   - **Latest Response**: See the most recent prompt-response pair
   - **All Responses**: Browse all executed prompts with download options
   - **Execution History**: View summary table of all executions

5. **Export results**:
   - Download individual prompt-response pairs as JSON
   - Export all responses as a comprehensive JSON file
   - Each download includes metadata (model, timing, etc.)

> üí° Supports both `/v1/chat/completions` and `/v1/completions` endpoints for maximum compatibility with local LLM servers.

---

## üõ†Ô∏è **Tips & Best Practices**

- **Start small**: Test with 20‚Äì50 texts before scaling up.
- **Use consistent Llama.cpp models**: Don't mix embeddings from different models.
- **Name your files clearly**: Include model name, date, and parameters.
- **Clusters are not final**: Adjust tree height or cluster count until groups make sense.
- **Keyword extraction improves with cluster quality**: Refine clustering if keywords seem off-topic.
- **Always validate LLM outputs**: The app prepares prompts‚Äî**you** interpret results.
- **Monitor execution times**: Large prompts or complex models may take longer to process.
- **Save responses frequently**: Download important results to avoid data loss.

---

## ‚ùì **Troubleshooting**

| Issue | Solution |
|------|--------|
| "No embeddings found" | Go back to **Data Management** and upload/generate data |
| Clustering fails | Ensure ‚â•2 embeddings; check for `NaN` in text |
| Keywords seem irrelevant | Try lowering `max_df` or increasing `min_df` |
| App is slow | Use smaller model (e.g., Gemma 300M) or reduce text length |
| JSON download fails | Ensure all integers are Python-native (app handles this automatically now) |
| "No prompts found" | Go to **Step 3b - LLM Prompt Generation** and generate prompts first |
| LLM server connection fails | Check server URL and ensure llama.cpp server is running with `--chat-format` |
| Prompt execution times out | Increase timeout or use smaller model/max_tokens setting |

    """)


if __name__ == '__main__':
    main()

